{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe288d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import h5py as h5\n",
    "from glob import glob\n",
    "import requests\n",
    "import json\n",
    "import time\n",
    "from collections import defaultdict\n",
    "import os\n",
    "import pyarrow.feather as feather\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d169954e",
   "metadata": {},
   "outputs": [],
   "source": [
    "species = \"human\"\n",
    "h5_version = 2.2\n",
    "gsm4sig_version = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a7a3e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = h5.File(f\"../{species}_gene_v{h5_version}.h5\", \"r\")\n",
    "genes = [x.decode('UTF-8') for x in f['meta/genes/symbol']]\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b87f60f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "gsm4sig = pd.read_csv(f\"data/{species}_gsm4sig_v{gsm4sig_version}.csv\", index_col=0)\n",
    "gsm4sig[\"sig_num\"] = gsm4sig.groupby(\"series_id\").cumcount()+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22eb2f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(gsm4sig[\"series_id\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "091e0947",
   "metadata": {},
   "outputs": [],
   "source": [
    "gsm4sig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d61359b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "gse2title = defaultdict(lambda: \"\")\n",
    "gse_trunc = gsm4sig[\"series_id\"].str.split(\"-\").map(lambda x: x[0]) #keeps only subseries if sub/superseries exists\n",
    "api_key = os.getenv('API_KEY')              #use of api_key increases rate limit 3 -> 10 req/s\n",
    "for i, gse in tqdm(enumerate(gse_trunc.unique())):                        #find title of the study via the ncbi e-utilities api                          #for use as part of signature name\n",
    "    try:\n",
    "        if ',' in gse:\n",
    "            gse = gse.split(',')[0]\n",
    "        url = \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/\"\n",
    "        params = {\"db\":\"gds\", \n",
    "                  \"term\": f\"{gse}[ACCN]\", \n",
    "                  \"retmax\":1, \n",
    "                  \"retmode\":\"json\",\n",
    "                  \"api_key\":api_key}\n",
    "        r = requests.get(url + \"esearch.fcgi\", params=params)\n",
    "        params[\"id\"] = r.json()[\"esearchresult\"][\"idlist\"][0]\n",
    "        r = requests.get(url + \"esummary.fcgi\", params=params)\n",
    "        gse2title[gse] = r.json()['result'][params[\"id\"]][\"title\"]\n",
    "        del params[\"id\"]\n",
    "        time.sleep(1/6)\n",
    "    except:\n",
    "        print(i, gse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de06834a",
   "metadata": {},
   "outputs": [],
   "source": [
    "set(gse2title.keys()).symmetric_difference(set(gse_trunc.unique())) \n",
    "#studies in which the study name was not found (data privatized, etc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d1b99bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(f\"{species}_gse2title.json\",\"w\")\n",
    "f.write(json.dumps(gse2title))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e680a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "logFC = pd.DataFrame(index = genes)\n",
    "pval = pd.DataFrame(index = genes)\n",
    "adjpval = pd.DataFrame(index = genes)\n",
    "\n",
    "glob_csv = glob(f\"./results/{species}/*.csv\")\n",
    "for n, file in enumerate(glob_csv):                                                 #aggregate logFC, p-value, and adjusted\n",
    "    if n % 200 == 0: print(n, \"done\")                                               #p-value from each individual signature\n",
    "    if n % 500 == 0 and n != 0:                                                     #file into their own dataframe files for\n",
    "        logFC.to_csv(f\"./results/sub_{species}/{species}_logFC_chunk_{n}.csv\")      #easy query later with appyter.\n",
    "        pval.to_csv(f\"./results/sub_{species}/{species}_pval_chunk_{n}.csv\")        #files written at 500 signature increments\n",
    "        adjpval.to_csv(f\"./results/sub_{species}/{species}_adjpval_chunk_{n}.csv\")  #to speed up aggregation\n",
    "        \n",
    "        logFC = pd.DataFrame(index = genes)\n",
    "        pval = pd.DataFrame(index = genes)\n",
    "        adjpval = pd.DataFrame(index = genes)\n",
    "        \n",
    "    idx, gse = file.split(\"\\\\\")[-1].split(\"_\")[:2]\n",
    "    gse = gse.split(\"-\")[0]\n",
    "    colname = f\"{gse2title[gse]} {gse}_{gsm4sig.at[int(idx), 'sig_num']}\"\n",
    "    file_df = pd.read_csv(file, index_col=0)\n",
    "    \n",
    "    logFC = (logFC\n",
    "             .join(file_df[\"logFC\"], how=\"left\")\n",
    "             .rename(columns={\"logFC\":colname})\n",
    "             .fillna(0))\n",
    "\n",
    "    pval = (pval\n",
    "            .join(file_df[\"P.Value\"], how=\"left\")\n",
    "            .rename(columns={\"P.Value\":colname})\n",
    "            .fillna(1))\n",
    "    \n",
    "    adjpval = (adjpval\n",
    "               .join(file_df[\"adj.P.Val\"], how=\"left\")\n",
    "               .rename(columns={\"adj.P.Val\":colname})\n",
    "               .fillna(1))    \n",
    "    \n",
    "logFC.to_csv(f\"./results/sub_{species}/{species}_logFC_chunk_end.csv\")\n",
    "pval.to_csv(f\"./results/sub_{species}/{species}_pval_chunk_end.csv\")\n",
    "adjpval.to_csv(f\"./results/sub_{species}/{species}_adjpval_chunk_end.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91103ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_type = [(\"logFC\", \"fc\"), (\"pval\", \"pval\"), (\"adjpval\",\"adjpval\")]\n",
    "for data, title in data_type:                                                    #500-increment files for logFC, p-value\n",
    "    glob_chunk = sorted(glob(f\"results/sub_{species}/{species}_{data}_*.csv\"),   #and adjusted p-value aggregated together\n",
    "                        key=os.path.getmtime)                                    #for one file each\n",
    "    pd.concat((pd.read_csv(file, index_col=0) for file in glob_chunk), axis=1).to_csv(f\"all_{species}_{title}.csv\")\n",
    "    print(\"done with\", data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39cb9d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "species_list = [\"human\", \"mouse\"]\n",
    "data_list = [\"fc\", \"pval\", \"adjpval\"]\n",
    "\n",
    "for s in species_list:                                                           #csv files written as feather files to \n",
    "    for dat in data_list:                                                        #speed up reading speed during appyter query\n",
    "        df = pd.read_csv(f\"all_{s}_{dat}.csv\", index_col=0)\n",
    "        feather.write_feather(\n",
    "            df.T.reset_index().rename(columns={'index': 'sig'}), \n",
    "            f\"all_{s}_{dat}.feather\"\n",
    "        )\n",
    "        print(f\"all_{s}_{dat}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e915b5c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "species_list = [\"human\", \"mouse\"]\n",
    "data_list = [\"fc\", \"pval\", \"adjpval\"]\n",
    "num_splits = 350\n",
    "\n",
    "for s in species_list:                                                         #files split up into 350 files of ~100 genes\n",
    "    lookup_dict = {}                                                           #each in order to speed up reading during appyter\n",
    "    for n, dat in enumerate(data_list):                                        #query and make lookup json file to identify\n",
    "        big_df = pd.read_csv(f\"all_{s}_{dat}.csv\", index_col = 0)              #correct file to read given a queried gene\n",
    "        dfs = np.array_split(big_df, num_splits)\n",
    "        for i, df in enumerate(dfs):\n",
    "            if n == 0: lookup_dict.update({idx:i for idx in df.index})\n",
    "            df.T.to_csv(f\"split_datafiles/{s}/{s}_{dat}/{s}_{dat}_{i}.csv\")\n",
    "        print(f'done with {s} {dat}')\n",
    "    \n",
    "    with open(f'{s}_lookup.json', 'w') as f:\n",
    "        json.dump(lookup_dict, f)   \n",
    "    print(f'done with {s}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "094be1c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Signature statistics for {species}:\\n\")                                #generate summary statistics for the given \n",
    "print(f\"Total Signatures: {len(gsm4sig['extrap_score'])}\")                     #human/mouse signatures\n",
    "print(f\"High quality signatures (score=0,1): \\\n",
    "{sum(gsm4sig['extrap_score'].value_counts()[[0,1]])/sum(gsm4sig['extrap_score'].value_counts())*100:.2f}%\")\n",
    "print(f\"Breakdown by extrap_score:\\n{gsm4sig['extrap_score'].value_counts()}\")\n",
    "plt.bar(gsm4sig[\"extrap_score\"].value_counts().index, gsm4sig[\"extrap_score\"].value_counts())\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
